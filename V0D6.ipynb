{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    logging    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if GPU is being used for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3-2-1B\"\n",
    "output_dir = \"./llama-3.2-banking77-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"legacy-datasets/banking77\")\n",
    "print(f\"Dataset loaded: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampled = dataset['train']\n",
    "val_sampled = dataset['test'].shuffle(seed=42).select(range(50))\n",
    "\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_sampled,\n",
    "    \"test\": val_sampled\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset with train and val data: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert labelled data into text as \"Category xx\"\n",
    "\n",
    "label_description = {}\n",
    "for i in range(77):\n",
    "    label_description[i] = f\"Category {i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing huggingface token\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    formatted_inputs = []\n",
    "\n",
    "    for text, label_id in zip(examples[\"text\"], examples[\"label\"]):\n",
    "        label_text = label_description[label_id]\n",
    "\n",
    "        prompt = f\"### Instruction: Classify the following bank customer query into the appropriate category.\\n\\n### Input: {text}\\n\\n### Response: {label_text}\"\n",
    "        formatted_inputs.append(prompt)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        formatted_inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenized Dataset with Train and Val data: {tokenized_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the LLAMA 3.2 1B Model for fine-tuning\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                    # rank dimension\n",
    "    lora_alpha=8,           # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # attention layers to fine-tune\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"Model loaded with LoRA configuration: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} ({100 * trainable_params / all_params:.2f}% of all params)\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # we're doing causal language modeling not masked language modeling\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Saved model and run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3-2-1B\"\n",
    "lora_adapter_path = \"./llama-3.2-banking77-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    # Format input just like during training\n",
    "    prompt = f\"### Instruction: Classify the following bank customer query into the appropriate category. \\n\\n### Input: {text}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate prediction\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=20,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "\n",
    "    # Decode the prediction\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the actual response part (after \"### Response:\")\n",
    "    try:\n",
    "        response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    except:\n",
    "        response = decoded_output  # Fallback if formatting is unexpected\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "text_list = []\n",
    "actual_category_list = []\n",
    "predicted_category_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(test_dataset):\n",
    "    text = example[\"text\"]\n",
    "    text_list.append(text)\n",
    "\n",
    "    actual_category = label_description[example[\"label\"]]\n",
    "    actual_category_list.append(actual_category)\n",
    "\n",
    "    # Get model prediction\n",
    "    predicted_category = predict_category(text)\n",
    "    predicted_category_list.append(predicted_category)\n",
    "\n",
    "    # Count correct predictions\n",
    "    if predicted_category.strip() == actual_category.strip():\n",
    "        correct += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Text\":text_list, \"Actual_Category\":actual_category_list, \"Predicted_Category\":predicted_category_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_category(text):\n",
    "    match = re.search(r'Category\\s+(\\d+\\s*:?)', text)  # Match \"Category xx\" with 1 or 2 digits\n",
    "    if match:\n",
    "        category_match = match.group(1)\n",
    "        category_clean = category_match.rstrip(': ')\n",
    "        return f\"Category {category_clean}\"\n",
    "    else:\n",
    "        return None  # Return None if no match\n",
    "\n",
    "# Create new column with extracted category\n",
    "df['Final_Category'] = df['Predicted_Category'].apply(extract_category)\n",
    "\n",
    "# Display result\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = df['Actual_Category'] == df[\"Final_Category\"]\n",
    "accuracy = matches.mean() * 100\n",
    "\n",
    "total_records = len(df)\n",
    "matching_records = matches.sum()\n",
    "\n",
    "print(f\"Matching records: {matching_records} out of {total_records}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"FinalOutput_01042025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(df[\"Actual_Category\"], df[\"Final_Category\"])\n",
    "\n",
    "# Compute precision, recall, and F1-score for each category\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(df[\"Actual_Category\"], df[\"Final_Category\"], average=None)\n",
    "\n",
    "# Compute weighted precision and recall\n",
    "weighted_precision, weighted_recall, _ = precision_recall_fscore_support(\n",
    "    df[\"Actual_Category\"], df[\"Final_Category\"], average=\"weighted\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision for each category: {dict(zip(sorted(df['Actual_Category'].unique()), precision))}\")\n",
    "print(f\"Recall for each category: {dict(zip(sorted(df['Actual_Category'].unique()), recall))}\")\n",
    "print(f\"Weighted Precision: {weighted_precision:.2f}\")\n",
    "print(f\"Weighted Recall: {weighted_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = sorted(df[\"Actual_Category\"].unique())\n",
    "\n",
    "result_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in unique_categories:\n",
    "    category_count = (df[\"Actual_Category\"] == category).sum()\n",
    "    match_count = ((df[\"Actual_Category\"] == category) & (df[\"Actual_Category\"] == df[\"Final_Category\"])).sum()\n",
    "    match_percentage = (match_count / category_count) * 100 if category_count > 0 else 0\n",
    "\n",
    "    result_data.append({\n",
    "        \"Category\": category,\n",
    "        \"Number_of_Samples\": category_count,\n",
    "        \"Match_count\": match_count,\n",
    "        \"Match_Percent\": round(match_percentage, 2)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Number_of_Samples</th>\n",
       "      <th>Match_count</th>\n",
       "      <th>Match_Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Category 0</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>92.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Category 1</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>82.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Category 10</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>82.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Category 11</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Category 12</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Category 13</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Category 14</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Category 15</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Category 16</td>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Category 17</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category  Number_of_Samples  Match_count  Match_Percent\n",
       "0   Category 0                 40           37           92.5\n",
       "1   Category 1                 40           33           82.5\n",
       "2  Category 10                 40           33           82.5\n",
       "3  Category 11                 40           32           80.0\n",
       "4  Category 12                 40           36           90.0\n",
       "5  Category 13                 40           35           87.5\n",
       "6  Category 14                 40           35           87.5\n",
       "7  Category 15                 40           32           80.0\n",
       "8  Category 16                 40           34           85.0\n",
       "9  Category 17                 40           38           95.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
